{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "480066b8",
   "metadata": {},
   "source": [
    "<h1 align='center'>Synthetic Data Generation and Unsloth Tutorial</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988e8e0a-e95f-4f46-840f-944bd7335754",
   "metadata": {},
   "source": [
    "## ğŸ“š Table of Contents:\n",
    "\n",
    "- [Synthetic Data Kit: Data Generation](#synthetic-data-generation)\n",
    "- [Unsloth: Fine-Tuning and saving the model](#fine-tuning)\n",
    "\n",
    "## Synthetic Data Generation\n",
    "\n",
    "In this section, we use the CLI from synthetic-data-kit to generate datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roro3n17eek",
   "metadata": {},
   "source": [
    "### Testing Synthetic Data Kit Command\n",
    "\n",
    "Please make sure you are running vllm by opening a terminal and typing `vllm serve Unsloth/Llama-3.3-70B-Instruct   --port 8001   --max-model-len 48000   --gpu-memory-utilization 0.85`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sv060626q8r",
   "metadata": {},
   "source": [
    "### Exploring Synthetic Data Kit CLI\n",
    "\n",
    "This command displays the help menu for the `synthetic-data-kit` CLI tool, showing available commands:\n",
    "- **system-check**: Verify LLM provider server is running\n",
    "- **ingest**: Parse documents (PDF, HTML, YouTube, etc.) into clean text\n",
    "- **create**: Generate synthetic content (Q&A pairs, instructions, etc.) using LLM\n",
    "- **curate**: Filter and clean generated content based on quality scores\n",
    "- **save-as**: Convert data to different formats (fine-tuning format, JSON, etc.)\n",
    "- **server**: Launch web interface for the toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f33f54be-101e-4e96-b892-b05ed5a08a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m \u001b[0m\u001b[1;33mUsage: \u001b[0m\u001b[1msynthetic-data-kit [OPTIONS] COMMAND [ARGS]...\u001b[0m\u001b[1m                         \u001b[0m\u001b[1m \u001b[0m\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      " A toolkit for preparing synthetic datasets for fine-tuning LLMs                \n",
      "                                                                                \n",
      "\u001b[2mâ•­â”€\u001b[0m\u001b[2m Options \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-config\u001b[0m              \u001b[1;32m-c\u001b[0m      \u001b[1;33mPATH\u001b[0m  Path to configuration file               \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-install\u001b[0m\u001b[1;36m-completion\u001b[0m          \u001b[1;33m    \u001b[0m  Install completion for the current       \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m                                     shell.                                   \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-show\u001b[0m\u001b[1;36m-completion\u001b[0m             \u001b[1;33m    \u001b[0m  Show completion for the current shell,   \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m                                     to copy it or customize the              \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m                                     installation.                            \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-help\u001b[0m                        \u001b[1;33m    \u001b[0m  Show this message and exit.              \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "\u001b[2mâ•­â”€\u001b[0m\u001b[2m Commands \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36msystem-check \u001b[0m\u001b[1;36m \u001b[0m Check if the selected LLM provider's server is running.       \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36mingest       \u001b[0m\u001b[1;36m \u001b[0m Parse documents (PDF, HTML, YouTube, DOCX, PPT, TXT) into     \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36m              \u001b[0m clean text.                                                   \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36mcreate       \u001b[0m\u001b[1;36m \u001b[0m Generate content from text using local LLM inference.         \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36mcurate       \u001b[0m\u001b[1;36m \u001b[0m Clean and filter content based on quality.                    \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36msave-as      \u001b[0m\u001b[1;36m \u001b[0m Convert to different formats for fine-tuning.                 \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m \u001b[1;36mserver       \u001b[0m\u001b[1;36m \u001b[0m Start a web interface for the Synthetic Data Kit.             \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!synthetic-data-kit --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lou0vs3mbib",
   "metadata": {},
   "source": [
    "### Verifying LLM Server Status\n",
    "\n",
    "This command checks if the vLLM server is running and accessible at `http://localhost:8001/v1`. It displays:\n",
    "- Server status and endpoint\n",
    "- Available models (here: Unsloth/Llama-3.3-70B-Instruct)\n",
    "- Model configuration (max context length: 48000 tokens)\n",
    "\n",
    "The system is configured to use the vLLM provider as specified in `tutorial_config.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6a6d30e-d3cc-43e5-b1dd-189d393aa8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: tutorial_config.yaml\n",
      "Config has LLM provider set to: vllm\n",
      "\u001b[1;34mEnvironment variable check:\u001b[0m\n",
      "API_ENDPOINT_KEY: Not found\n",
      "get_llm_provider returning: vllm\n",
      "\u001b[?25l\u001b[31mL vLLM server is not available at \u001b[0m\u001b[4;94mhttp://localhost:8001/v1\u001b[0m\n",
      "\u001b[2KError: \u001b[1;35mHTTPConnectionPool\u001b[0m\u001b[1m(\u001b[0m\u001b[33mhost\u001b[0m=\u001b[32m'localhost'\u001b[0m, \u001b[33mport\u001b[0m=\u001b[1;36m8001\u001b[0m\u001b[1m)\u001b[0m: Max retries exceeded \n",
      "with url: \u001b[35m/v1/\u001b[0m\u001b[95mmodels\u001b[0m \u001b[1m(\u001b[0mCaused by \n",
      "\u001b[1;35mNewConnectionError\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32murllib3.connection.HTTPConnection\u001b[0m\u001b[32m object at \u001b[0m\n",
      "\u001b[32m0x7f6c06938440\u001b[0m\u001b[32m>\u001b[0m\u001b[32m: Failed to establish a new connection: \u001b[0m\u001b[32m[\u001b[0m\u001b[32mErrno 111\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Connection \u001b[0m\n",
      "\u001b[32mrefused'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
      "\u001b[2Kmâ ‹\u001b[0m Checking vLLM server at http://localhost:8001/v1...\n",
      "\u001b[33mTo start the server, run:\u001b[0m\n",
      "\u001b[2K\u001b[1;34mvllm serve Unsloth/Llama-\u001b[0m\u001b[1;36m3.3\u001b[0m\u001b[1;34m-70B-Instruct --port \u001b[0m\u001b[1;36m8001\u001b[0m\n",
      "\u001b[2K\u001b[32mâ ‹\u001b[0m Checking vLLM server at http://localhost:8001/v1...v1...\u001b[0m\n",
      "\u001b[1A\u001b[2K"
     ]
    }
   ],
   "source": [
    "!synthetic-data-kit -c tutorial_config.yaml system-check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lj8z1mjus4",
   "metadata": {},
   "source": [
    "### Creating Project Directory Structure\n",
    "\n",
    "This command creates a well-organized directory structure for the logical reasoning project:\n",
    "- `sources/`: Store original source documents (PDFs, etc.)\n",
    "- `data/input/`: Input files for processing\n",
    "- `data/parsed/`: Parsed text files after document ingestion\n",
    "- `data/generated/`: Generated synthetic Q&A pairs\n",
    "- `data/curated/`: Quality-filtered data after curation\n",
    "- `data/final/`: Final formatted data ready for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "289ebeda-4d12-4466-a6df-4a82bd4a175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash -c 'mkdir -p logical_reasoning/{sources,data/{input,parsed,generated,curated,final}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0vu978x4i9hb",
   "metadata": {},
   "source": [
    "### Navigating to Project Directory\n",
    "\n",
    "Changes the current working directory to `logical_reasoning/` where all subsequent operations will take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "264477d5-7d25-4fac-99a5-5497d7dcd753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/AAIPL/logical_reasoning\n"
     ]
    }
   ],
   "source": [
    "cd logical_reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ukh0uhxa8ca",
   "metadata": {},
   "source": [
    "### Downloading Source Documents\n",
    "\n",
    "Downloads two PDF documents related to logical reasoning and liar/truth puzzles:\n",
    "1. \"Logical Reasoning\" textbook from CSU Sacramento\n",
    "2. \"Liar and Truth Teller Puzzles\" from UMass\n",
    "\n",
    "These documents will serve as the knowledge base for generating synthetic training data. The `-q` flag runs wget in quiet mode, and `--show-progress` displays a progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fa11bee-b9b0-470a-875c-673bc88d3cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logical-reasoning-1 100%[===================>]   5.52M  1.49MB/s    in 4.0s    \n",
      "Liar_Truth.pdf      100%[===================>] 327.61K  --.-KB/s    in 0.1s    \n"
     ]
    }
   ],
   "source": [
    "!wget -P sources/ -q --show-progress   \"https://www.csus.edu/faculty/d/dowden/_internal/_documents/logical-reasoning-12.pdf\"   \"https://people.cs.umass.edu/~pthomas/solutions/Liar_Truth.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gcvmiim3d49",
   "metadata": {},
   "source": [
    "### Copying Source Files to Input Directory\n",
    "\n",
    "Copies all downloaded source documents from `sources/` to `data/input/` to prepare them for the ingestion pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26d79b79-1f40-4ede-bc54-60496ccf65e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp sources/* data/input/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ttgi76hqiv",
   "metadata": {},
   "source": [
    "### Ingesting and Parsing Documents\n",
    "\n",
    "This command processes the PDF files in `data/input/` using the synthetic-data-kit's **ingest** command:\n",
    "- Extracts text content from PDFs\n",
    "- Cleans and normalizes the text\n",
    "- Saves parsed text files to `data/parsed/`\n",
    "\n",
    "The output shows successful processing of 2 PDF files (Liar_Truth.pdf and logical-reasoning-12.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c71729fe-a9f2-495c-9ef5-6f58bcaa1232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "\u001b[34mProcessing directory: \u001b[0m\u001b[1;34m.\u001b[0m\u001b[1;35m/data/input/\u001b[0m\n",
      "\u001b[34mFound \u001b[0m\u001b[1;36m2\u001b[0m\u001b[34m supported files to process\u001b[0m\n",
      "\u001b[32mâœ“ Liar_Truth.pdf\u001b[0m\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P12' is an invalid float value\n",
      "\u001b[32mâœ“ logical-reasoning-\u001b[0m\u001b[1;36m12.\u001b[0m\u001b[32mpdf\u001b[0m\n",
      "\n",
      "\u001b[1m==================================================\u001b[0m\n",
      "\u001b[1;34mProcessing Summary:\u001b[0m\n",
      "Total files: \u001b[1;36m2\u001b[0m\n",
      "\u001b[32mSuccessful: \u001b[0m\u001b[1;36m2\u001b[0m\n",
      "\u001b[32mFailed: \u001b[0m\u001b[1;36m0\u001b[0m\n",
      "\u001b[1m==================================================\u001b[0m\n",
      "\u001b[32mâœ… All files processed successfully!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!synthetic-data-kit ingest ./data/input/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s722mc1b399",
   "metadata": {},
   "source": [
    "### Generating Synthetic Q&A Pairs\n",
    "\n",
    "This command uses the synthetic-data-kit's **create** command to generate Q&A pairs from the parsed text:\n",
    "- Reads parsed text files from `data/parsed/`\n",
    "- Uses the vLLM provider with Llama-3.3-70B-Instruct model\n",
    "- Generates 50 Q&A pairs per file (`--num-pairs 50`)\n",
    "- Type is set to `qa` for question-answer pair generation\n",
    "- Outputs are saved to `data/generated/`\n",
    "\n",
    "The process chunks the text and generates questions with corresponding answers. This took about 10 minutes for the full run. Use `--verbose` flag to see detailed progress or reduce `--num-pairs` for faster testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe68a35-cbdd-453b-93fd-0d85e5489cea",
   "metadata": {},
   "source": [
    "Note: This will take about 10 minutes, set `--verbose` flag to see progress or reduce the `num-pairs` for a faster test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af209d32-0a99-4365-bee0-18a14af2c9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: ../tutorial_config.yaml\n",
      "Config has LLM provider set to: vllm\n",
      "get_llm_provider returning: vllm\n",
      "\u001b[32mğŸ”— Using vllm provider\u001b[0m\n",
      "\u001b[31mâŒ Error: VLLM server not available at \u001b[0m\u001b[4;94mhttp://localhost:8001/v1\u001b[0m\n",
      "\u001b[33mPlease start the VLLM server with:\u001b[0m\n",
      "\u001b[1;34mvllm serve Unsloth/Llama-\u001b[0m\u001b[1;36m3.3\u001b[0m\u001b[1;34m-70B-Instruct\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!synthetic-data-kit -c ../tutorial_config.yaml create ./data/parsed/ --type qa --num-pairs 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lr9f7y5tqur",
   "metadata": {},
   "source": [
    "### Curating and Quality Filtering\n",
    "\n",
    "This command uses the **curate** function to filter generated Q&A pairs based on quality:\n",
    "- Evaluates each Q&A pair using quality metrics\n",
    "- Filters pairs with quality score above threshold (7.0/10)\n",
    "- Removes low-quality, inconsistent, or malformed pairs\n",
    "- Saves curated data to `data/curated/`\n",
    "\n",
    "This ensures only high-quality synthetic data is used for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cabcce1-73f4-453b-a12e-7374f044462b",
   "metadata": {},
   "source": [
    "Note: This will also take about 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44510f47-9cbd-4890-984f-3980c145e554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: ../tutorial_config.yaml\n",
      "Config has LLM provider set to: vllm\n",
      "get_llm_provider returning: vllm\n",
      "\u001b[32mğŸ”— Using vllm provider\u001b[0m\n",
      "\u001b[31mâŒ Error: VLLM server not available at \u001b[0m\u001b[4;94mhttp://localhost:8001/v1\u001b[0m\n",
      "\u001b[33mPlease start the VLLM server with:\u001b[0m\n",
      "\u001b[1;34mvllm serve Unsloth/Llama-\u001b[0m\u001b[1;36m3.3\u001b[0m\u001b[1;34m-70B-Instruct\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!synthetic-data-kit -c ../tutorial_config.yaml curate ./data/generated/ --threshold 7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awlg3cppl1t",
   "metadata": {},
   "source": [
    "### Converting to Fine-Tuning Format\n",
    "\n",
    "This command uses the **save-as** function to convert curated Q&A pairs to fine-tuning format:\n",
    "- Reads curated JSON files from `data/curated/`\n",
    "- Converts to format `ft` (fine-tuning format with messages structure)\n",
    "- Outputs are saved to `data/final/` with proper conversation format\n",
    "- The resulting format is compatible with standard fine-tuning pipelines\n",
    "\n",
    "Successfully converted 2 files to fine-tuning format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67ed73a8-7f3e-4a6d-9bdb-ba190b8b1bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "Loading config from: /usr/local/lib/python3.12/dist-packages/synthetic_data_kit/config.yaml\n",
      "Config has LLM provider set to: api-endpoint\n",
      "\u001b[34mProcessing directory: \u001b[0m\u001b[1;34m.\u001b[0m\u001b[1;35m/data/curated/\u001b[0m\u001b[34m for format conversion to ft\u001b[0m\n",
      "\u001b[33mNo supported files found in .\u001b[0m\u001b[35m/data/curated/\u001b[0m\n",
      "\u001b[33mFor save-as: looking for .json files with cleaned QA pairs\u001b[0m\n",
      "\u001b[32mâœ… All files converted successfully!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!synthetic-data-kit save-as ./data/curated/ --format ft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3akj2fu3yr8",
   "metadata": {},
   "source": [
    "### Loading and Converting Data to HuggingFace Dataset\n",
    "\n",
    "This cell performs comprehensive data processing:\n",
    "\n",
    "1. **Finding Files**: Locates all JSON files in `data/final/` directory\n",
    "2. **Loading Data**: Reads each JSON file containing fine-tuning formatted data\n",
    "3. **Format Conversion**: Extracts user and assistant messages from the fine-tuning format\n",
    "4. **Structuring Conversations**: Creates a standardized conversation format with role-content pairs\n",
    "5. **Creating Dataset**: Converts the processed data into a HuggingFace Dataset object\n",
    "\n",
    "The output shows 74 total conversations were successfully loaded and formatted. The preview displays a sample conversation showing either a knight-and-knave logic puzzle with its solution or a deduction puzzle about the Logical Reasoning book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dbbd1b2-a4f2-46b7-bd01-6841abde7637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ Total conversations: 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Invalid key: 0 is out of bounds for size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     44\u001b[39m dataset = Dataset.from_list(all_data)\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# ===== STEP 4: Preview the data =====\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28mprint\u001b[39m(json.dumps(\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m, indent=\u001b[32m2\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py:2867\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2865\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33marrow\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpandas\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpolars\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   2866\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m Column(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[32m-> \u001b[39m\u001b[32m2867\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py:2848\u001b[39m, in \u001b[36mDataset._getitem\u001b[39m\u001b[34m(self, key, **kwargs)\u001b[39m\n\u001b[32m   2846\u001b[39m format_kwargs = format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m   2847\u001b[39m formatter = get_formatter(format_type, features=\u001b[38;5;28mself\u001b[39m._info.features, **format_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2848\u001b[39m pa_subtable = \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2849\u001b[39m formatted_output = format_table(\n\u001b[32m   2850\u001b[39m     pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\n\u001b[32m   2851\u001b[39m )\n\u001b[32m   2852\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/datasets/formatting/formatting.py:612\u001b[39m, in \u001b[36mquery_table\u001b[39m\u001b[34m(table, key, indices)\u001b[39m\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    611\u001b[39m     size = indices.num_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table.num_rows\n\u001b[32m--> \u001b[39m\u001b[32m612\u001b[39m     \u001b[43m_check_valid_index_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[38;5;66;03m# Query the main table\u001b[39;00m\n\u001b[32m    614\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/datasets/formatting/formatting.py:552\u001b[39m, in \u001b[36m_check_valid_index_key\u001b[39m\u001b[34m(key, size)\u001b[39m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (key < \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key + size < \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (key >= size):\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is out of bounds for size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n",
      "\u001b[31mIndexError\u001b[39m: Invalid key: 0 is out of bounds for size 0"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "data_dir = \"./data/final\"  # Change this to your data directory\n",
    "\n",
    "# ===== STEP 1: Find all FT files =====\n",
    "data_path = Path(data_dir)\n",
    "ft_files = glob.glob(str(data_path / \"*.json\"))\n",
    "\n",
    "# ===== STEP 2: Load and convert all files =====\n",
    "all_data = []\n",
    "\n",
    "for file_path in ft_files:\n",
    "    # Load the JSON file\n",
    "    with open(file_path, 'r') as f:\n",
    "        ft_data = json.load(f)\n",
    "    \n",
    "    # Convert each item\n",
    "    for item in ft_data:\n",
    "        if 'messages' not in item:\n",
    "            continue\n",
    "        \n",
    "        # Extract only user and assistant messages\n",
    "        conversation = []\n",
    "        for msg in item['messages']:\n",
    "            if msg['role'] == 'user' or msg['role'] == 'assistant':\n",
    "                conversation.append({\n",
    "                    \"role\": msg['role'],\n",
    "                    \"content\": msg['content']\n",
    "                })\n",
    "        \n",
    "        # Add to our data if we have at least one exchange\n",
    "        if len(conversation) > 0:\n",
    "            all_data.append({\n",
    "                \"conversations\": conversation\n",
    "            })\n",
    "\n",
    "print(f\"\\nğŸ¯ Total conversations: {len(all_data)}\")\n",
    "\n",
    "# ===== STEP 3: Create HuggingFace Dataset =====\n",
    "dataset = Dataset.from_list(all_data)\n",
    "\n",
    "# ===== STEP 4: Preview the data =====\n",
    "print(json.dumps(dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ec326-9902-49db-aee0-b84acfdfb2cb",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n",
    "\n",
    "### Note: Please remember to shutdown the vLLM instance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dulvo8ocyrb",
   "metadata": {},
   "source": [
    "### Importing Standard Libraries\n",
    "\n",
    "Imports essential Python libraries for fine-tuning:\n",
    "- `os`, `json`, `glob`: File system operations and JSON handling\n",
    "- `torch`: PyTorch deep learning framework\n",
    "- `shutil`: File operations\n",
    "- `Path`: Path manipulation\n",
    "- `Dataset`: HuggingFace datasets library for data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77d5db74-5070-45b7-bdae-f93d50909b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iwk3zcrzft",
   "metadata": {},
   "source": [
    "### Importing Unsloth and Training Libraries\n",
    "\n",
    "Imports specialized libraries for efficient fine-tuning:\n",
    "- `FastLanguageModel` from Unsloth: Optimized model loading and training\n",
    "- `get_chat_template`, `standardize_sharegpt`, `train_on_responses_only`: Chat formatting utilities\n",
    "- `SFTConfig`, `SFTTrainer`: Supervised fine-tuning configuration and trainer from TRL\n",
    "- `DataCollatorForSeq2Seq`: Handles batching and padding for sequence-to-sequence training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8adaf850-eb8a-476e-8d09-34c807c92e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "#### Unsloth: `hf_xet==1.1.10` and `ipykernel>6.30.1` breaks progress bars. Disabling for now in XET.\n",
      "#### Unsloth: To re-enable progress bars, please downgrade to `ipykernel==6.30.1` or wait for a fix to\n",
      "https://github.com/huggingface/xet-core/issues/526\n",
      "INFO 02-14 10:31:49 [__init__.py:225] Automatically detected platform rocm.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template, standardize_sharegpt, train_on_responses_only\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305661d0-5103-412c-9f33-7ad61cc288b3",
   "metadata": {},
   "source": [
    "## Setup Unsloth model and tokenizer for ROCm without bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "erpk4j0opb6",
   "metadata": {},
   "source": [
    "### Loading Llama-3.3-70B Model with LoRA\n",
    "\n",
    "This cell sets up the model for efficient fine-tuning on AMD ROCm hardware:\n",
    "\n",
    "**Model Configuration:**\n",
    "- Model: Llama-3.3-70B-Instruct (70 billion parameters)\n",
    "- Data type: bfloat16 for ROCm compatibility\n",
    "- No quantization (load_in_4bit=False) to avoid bitsandbytes dependency\n",
    "- Max sequence length: 1024 tokens\n",
    "\n",
    "**LoRA (Low-Rank Adaptation) Configuration:**\n",
    "- Rank (r): 64 - Higher rank for the large 70B model\n",
    "- Target modules: All attention and MLP layers (q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj)\n",
    "- LoRA alpha: 64\n",
    "- Dropout: 0 (no dropout)\n",
    "- Gradient checkpointing: \"unsloth\" for memory efficiency\n",
    "\n",
    "LoRA enables efficient fine-tuning by only training small adapter layers instead of the entire 70B model, making it feasible to train on a single AMD MI300X GPU with 192GB HBM3 memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5dffe96-b007-4220-b7b7-e40e219b267c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 30] Read-only file system: '/root/.cache/huggingface/hub'\n",
      "[2026-02-14 10:31:52] ERROR file_download.py:1556: Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 30] Read-only file system: '/root/.cache/huggingface/hub'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsloth: No config file found - are you sure the `model_name` is correct?\nIf you're using a model on your local device, confirm if the folder location exists.\nIf you're using a HuggingFace online model, check if it exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m dtype = torch.bfloat16  \u001b[38;5;66;03m# Explicit bfloat16 for ROCm\u001b[39;00m\n\u001b[32m      3\u001b[39m load_in_4bit = \u001b[38;5;28;01mFalse\u001b[39;00m  \n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model, tokenizer = \u001b[43mFastLanguageModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munsloth/Llama-3.3-70B-Instruct\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Explicit for ROCm\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… Loaded: Llama-3.3-70B-Instruct (bfloat16, ROCm compatible)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Add LoRA adapters\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/unsloth/models/loader.py:281\u001b[39m, in \u001b[36mFastLanguageModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, load_in_16bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, offload_embedding, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, qat_scheme, *args, **kwargs)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m both_exist:\n\u001b[32m    275\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    276\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnsloth: Your repo has a LoRA adapter and a base model.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m    277\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou have 2 files `config.json` and `adapter_config.json`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m    278\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWe must only allow one config file.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m    279\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease separate the LoRA and base models to 2 repos.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    280\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m model_types = \u001b[43mget_transformers_model_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model_types) == \u001b[32m1\u001b[39m:\n\u001b[32m    285\u001b[39m     model_type = model_types[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/hf_utils.py:112\u001b[39m, in \u001b[36mget_transformers_model_type\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\" Gets model_type from config file - can be PEFT or normal HF \"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    113\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsloth: No config file found - are you sure the `model_name` is correct?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m    114\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIf you\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre using a model on your local device, confirm if the folder location exists.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m    115\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIf you\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre using a HuggingFace online model, check if it exists.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    116\u001b[39m     )\n\u001b[32m    117\u001b[39m model_types = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftConfig\n",
      "\u001b[31mRuntimeError\u001b[39m: Unsloth: No config file found - are you sure the `model_name` is correct?\nIf you're using a model on your local device, confirm if the folder location exists.\nIf you're using a HuggingFace online model, check if it exists."
     ]
    }
   ],
   "source": [
    "max_seq_length = 1024\n",
    "dtype = torch.bfloat16  # Explicit bfloat16 for ROCm\n",
    "load_in_4bit = False  \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.3-70B-Instruct\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,  # Explicit for ROCm\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"âœ… Loaded: Llama-3.3-70B-Instruct (bfloat16, ROCm compatible)\")\n",
    "\n",
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,  # Higher rank for 70B model\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9exgyip7y8f",
   "metadata": {},
   "source": [
    "### Preparing Dataset with Chat Template\n",
    "\n",
    "This cell formats the dataset for fine-tuning:\n",
    "\n",
    "**Steps:**\n",
    "1. **Set Chat Template**: Applies Llama-3.1 chat template formatting\n",
    "2. **Configure Padding**: Sets pad token to eos token if not already set\n",
    "3. **Format Conversations**: The `formatting_prompts_func` function:\n",
    "   - Takes raw conversations from the dataset\n",
    "   - Applies the chat template to format them properly\n",
    "   - Validates conversation structure (list of dicts with role/content)\n",
    "   - Filters out malformed conversations\n",
    "4. **Standardize Format**: Uses `standardize_sharegpt` to normalize the data structure\n",
    "5. **Apply Formatting**: Maps the formatting function across all examples\n",
    "6. **Remove Empty**: Filters out any empty or invalid formatted texts\n",
    "\n",
    "The output shows 74 valid examples were successfully prepared. A sample of the formatted text is displayed, showing the proper Llama-3.1 chat template structure with system, user, and assistant headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496f9c09-609c-41f7-a7a7-c1f942405a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Prepare dataset with proper chat template and tensor compatibility\"\"\"\n",
    "print(\"ğŸ”§ Preparing dataset for training...\")\n",
    "\n",
    "# Set chat template\n",
    "tokenizer = get_chat_template(tokenizer, chat_template=\"llama-3.1\")\n",
    "\n",
    "# Ensure pad token is set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Formatting function that ensures proper tensor conversion\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = []\n",
    "    \n",
    "    for convo in convos:\n",
    "        # Ensure conversation is in correct format\n",
    "        if isinstance(convo, list) and all(isinstance(msg, dict) for msg in convo):\n",
    "            text = tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "            texts.append(text)\n",
    "        else:\n",
    "            print(f\"âš ï¸  Skipping malformed conversation: {type(convo)}\")\n",
    "            continue\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "dataset = dataset.filter(lambda x: len(x[\"text\"].strip()) > 0)\n",
    "\n",
    "print(f\"âœ… Prepared {len(dataset)} valid examples for training\")\n",
    "\n",
    "# Show sample\n",
    "if len(dataset) > 0:\n",
    "    print(f\"ğŸ“ Sample formatted text:\")\n",
    "    print(dataset[\"text\"][0][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otx9lfwgfmi",
   "metadata": {},
   "source": [
    "### Training the Model with ROCm-Optimized Settings\n",
    "\n",
    "This cell configures and executes the fine-tuning process:\n",
    "\n",
    "**Training Configuration (SFTConfig):**\n",
    "- **Batch size**: 64 per device - leveraging the AMD MI300X's massive 192GB HBM3 memory\n",
    "- **Gradient accumulation**: 1 step\n",
    "- **Warmup**: 5 steps\n",
    "- **Epochs**: 1 full pass through the dataset\n",
    "- **Learning rate**: 1e-4\n",
    "- **Optimizer**: adamw_8bit for memory efficiency\n",
    "- **Precision**: bf16 (bfloat16) for ROCm\n",
    "- **Gradient checkpointing**: Enabled for memory efficiency\n",
    "\n",
    "**Special Training Mode:**\n",
    "Uses `train_on_responses_only` to compute loss only on the assistant's responses, not on the user's questions. This focuses the model on learning to generate accurate answers rather than memorizing the input format.\n",
    "\n",
    "**Key Features:**\n",
    "- DataCollatorForSeq2Seq handles variable-length sequences with proper padding\n",
    "- No packing to preserve conversation structure\n",
    "- Single dataloader worker for ROCm stability\n",
    "- Gradient checkpointing via Unsloth for memory optimization\n",
    "\n",
    "The model is then trained on the 74 logical reasoning conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144231a7-313f-4db9-8f02-025148666732",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train model with ROCm-optimized settings\"\"\"\n",
    "# Ensure tokenizer has proper padding\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Setup trainer with ROCm-friendly settings and proper data handling\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    "    packing=False,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=32,  # ğŸš€ MI300X can handle this with 192GB HBM3!\n",
    "        gradient_accumulation_steps=2,   # Effective batch size = 32*2 = 64\n",
    "        warmup_steps=5,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=1e-4,\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",  # Pure torch optimizer\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"logical_reasoning_rocm_outputs\",\n",
    "        report_to=\"none\",\n",
    "        bf16=True,\n",
    "        dataloader_pin_memory=False,\n",
    "        remove_unused_columns=True,  # Remove unused columns to avoid tensor issues\n",
    "        gradient_checkpointing=True,\n",
    "        dataloader_num_workers=0,  # Single worker for ROCm stability\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Train only on responses\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_training(model)\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zlv8ydhyokk",
   "metadata": {},
   "source": [
    "### Saving the Fine-Tuned Model\n",
    "\n",
    "This cell saves the trained model in two formats:\n",
    "\n",
    "1. **LoRA Adapters** (`logical_reasoning_rocm_lora/`):\n",
    "   - Saves only the trained LoRA adapter weights (lightweight, ~few hundred MB)\n",
    "   - Can be loaded later with the base model\n",
    "   - Useful for sharing or deploying with the original base model\n",
    "\n",
    "2. **Merged Model** (`logical_reasoning_rocm_merged/`):\n",
    "   - Merges LoRA adapters back into the base model\n",
    "   - Creates a standalone model with all weights\n",
    "   - Saved in 16-bit precision for better quality\n",
    "   - Ready for immediate inference without loading adapters\n",
    "\n",
    "Both formats include the tokenizer configuration. The merged model is production-ready and can be used directly for generating answers to logical reasoning questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05799dd-25f8-4a03-8bb0-6b91d5d6dda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Save the trained model\"\"\"\n",
    "print(\"\\nğŸ’¾ SAVING ROCM-TRAINED MODEL\")\n",
    "\n",
    "# Save LoRA adapters\n",
    "lora_path = \"logical_reasoning_rocm_lora\"\n",
    "model.save_pretrained(lora_path)\n",
    "tokenizer.save_pretrained(lora_path)\n",
    "print(f\"âœ… LoRA adapters saved to: {lora_path}\")\n",
    "\n",
    "# Save merged model\n",
    "merged_path = \"logical_reasoning_rocm_merged\"\n",
    "print(\"ğŸ”„ Saving merged model...\")\n",
    "model.save_pretrained_merged(merged_path, tokenizer, save_method=\"merged_16bit\")\n",
    "print(f\"âœ… Merged model saved to: {merged_path}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ ROCM MODEL READY!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ee904c-4ee0-45b4-9a06-9d7f01b1b255",
   "metadata": {},
   "source": [
    "### Testing the Fine-Tuned Model\n",
    "You can do a quick test to check that the model is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dfae80-9635-4d62-b72a-b5671d7c62b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test the fine-tuned model with inference\"\"\"\n",
    "# Switch model to inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test question - a classic knight/knave logic puzzle\n",
    "test_question = \"A says 'B is a knave.' B says 'A and I are different types.' What are A and B?\"\n",
    "\n",
    "# Format the prompt using the chat template\n",
    "messages = [{\"role\": \"user\", \"content\": test_question}]\n",
    "input_text = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True  # Adds assistant header so model knows to respond\n",
    ")\n",
    "\n",
    "# Tokenize and move to GPU\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate response\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.1,      # Low temperature for more deterministic logical reasoning\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "# Decode only the generated part (exclude the input prompt)\n",
    "response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Question: {test_question}\\n\")\n",
    "print(f\"Answer: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f9b5d9-a31e-4824-80e4-26b75e68d401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97406d4-568e-40b7-84b5-6066d58a8d86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd4f4b9-38b6-4cf0-9049-2fa40cdbc386",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
